{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25269e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lilla.conte/.pyenv/versions/futureobs/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/lilla.conte/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/lilla.conte/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import CamembertTokenizer \n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "camembert_tokenizer = CamembertTokenizer.from_pretrained(\"camembert/camembert-base-ccnet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c4d264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per row: 581.93\n",
      "Median: 400.00\n",
      "Percentage of rows with > 500 tokens: 44.75%\n"
     ]
    }
   ],
   "source": [
    "# calculate average number of tokens per article (per row) in annotated 'rixes' sample data\n",
    "\n",
    "csv_file = '../data/annotated_dataset_deduped.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "if 'text' in df.columns:\n",
    "    token_counts = df['text'].fillna('').apply(lambda x: len(camembert_tokenizer.tokenize(x)))\n",
    "    \n",
    "    average = token_counts.mean()\n",
    "    median = np.median(token_counts)\n",
    "    over_500_count = (token_counts > 500).sum()\n",
    "    percent_over_500 = (over_500_count / len(token_counts)) * 100 if len(token_counts) > 0 else 0\n",
    "\n",
    "    print(f\"Average number of tokens per row: {average:.2f}\")\n",
    "    print(f\"Median: {median:.2f}\")\n",
    "    print(f\"Percentage of rows with > 500 tokens: {percent_over_500:.2f}%\")\n",
    "else:\n",
    "    print(\"Column 'text' not found in the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "138c6f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [08:04<00:00, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average percentage of rows with > 500 tokens across files: 11.23%\n",
      "\n",
      "True average tokens per row (across all files): 206.66\n",
      "Global median tokens per row: 84.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate average number of tokens per article (per row) in 'italiens/belges' dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "folder_path = '/store/retronews/etrangers'\n",
    "csv_files = [f for f in glob.glob(os.path.join(folder_path, '*.csv')) if not f.endswith(\"pattern.csv\")]\n",
    "\n",
    "total_tokens = 0\n",
    "total_rows = 0\n",
    "all_token_counts = []\n",
    "over_500_percentages = []\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    filename = os.path.basename(file)\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8', low_memory=False)\n",
    "\n",
    "        if 'text' in df.columns and not df.empty:\n",
    "            df['text'] = df['text'].fillna('')\n",
    "            token_counts = df['text'].apply(lambda x: len(camembert_tokenizer.tokenize(x)))\n",
    "\n",
    "            total_tokens += token_counts.sum()\n",
    "            total_rows += token_counts.count()\n",
    "            all_token_counts.extend(token_counts.tolist())\n",
    "\n",
    "            over_500 = (token_counts > 500).sum()\n",
    "            percent_over_500 = (over_500 / len(token_counts)) * 100 if len(token_counts) > 0 else 0\n",
    "            over_500_percentages.append(percent_over_500)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "if over_500_percentages:\n",
    "    avg_percent_over_500 = np.mean(over_500_percentages)\n",
    "    print(f\"\\nAverage percentage of rows with > 500 tokens across files: {avg_percent_over_500:.2f}%\")\n",
    "\n",
    "if total_rows > 0:\n",
    "    true_average = total_tokens / total_rows\n",
    "    global_median = np.median(all_token_counts)\n",
    "    print(f\"\\nTrue average tokens per row (across all files): {true_average:.2f}\")\n",
    "    print(f\"Global median tokens per row: {global_median:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4544e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant events: 176\n",
      "Percentage of relevant events: 59.66%\n"
     ]
    }
   ],
   "source": [
    "# calculate the number and percentage of events annotated as 'relevant' in annotated 'rixes' sample data\n",
    "\n",
    "csv_file = '../data/annotated_dataset_deduped.csv'\n",
    "df = pd.read_csv(csv_file) \n",
    "\n",
    "if 'relevant' in df.columns:\n",
    "    total_rows = len(df)\n",
    "    relevant_count = df['relevant'].sum()  # sums all 1s, since values are 0 or 1\n",
    "    relevant_percentage = (relevant_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "\n",
    "    print(f\"Number of relevant events: {relevant_count}\")\n",
    "    print(f\"Percentage of relevant events: {relevant_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"Column 'relevant' not found in the CSV file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9e890f-2e99-4107-a322-539c8027d2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/lilla.conte/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original sentence:\n",
      "Rixe sanglante Une rixe sanglante a éclaté la nuit dernière a Marseille dans le quartier avoisinant le vieux port, entre des ma rins français et des journaliers italiens.\n",
      "\n",
      "CamemBERT tokens:\n",
      "['▁R', 'ix', 'e', '▁sanglante', '▁Une', '▁ri', 'x', 'e', '▁sanglante', '▁a', '▁éclaté', '▁la', '▁nuit', '▁dernière', '▁a', '▁Marseille', '▁dans', '▁le', '▁quartier', '▁avoisinant', '▁le', '▁vieux', '▁port', ',', '▁entre', '▁des', '▁ma', '▁', 'rin', 's', '▁français', '▁et', '▁des', '▁journalier', 's', '▁italiens', '.']\n",
      "37\n",
      "\n",
      "NLTK tokens:\n",
      "['Rixe', 'sanglante', 'Une', 'rixe', 'sanglante', 'a', 'éclaté', 'la', 'nuit', 'dernière', 'a', 'Marseille', 'dans', 'le', 'quartier', 'avoisinant', 'le', 'vieux', 'port', ',', 'entre', 'des', 'ma', 'rins', 'français', 'et', 'des', 'journaliers', 'italiens', '.']\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# compare bert tokenizer with ntlk tokenizer\n",
    "\n",
    "from transformers import CamembertTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "doc = \"Rixe sanglante Une rixe sanglante a éclaté la nuit dernière a Marseille dans le quartier avoisinant le vieux port, entre des ma rins français et des journaliers italiens.\"\n",
    "\n",
    "camembert_tokens = camembert_tokenizer.tokenize(doc)\n",
    "nltk_tokens = word_tokenize(doc)\n",
    "\n",
    "# results\n",
    "print(\"\\nOriginal sentence:\")\n",
    "print(doc)\n",
    "\n",
    "print(\"\\nCamemBERT tokens:\")\n",
    "print(camembert_tokens)\n",
    "print(len(camembert_tokens))\n",
    "\n",
    "print(\"\\nNLTK tokens:\")\n",
    "print(nltk_tokens)\n",
    "print(len(nltk_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8b0cd67-507d-45b2-9c6b-13e756abe0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows with < 13 words: 0.34%\n"
     ]
    }
   ],
   "source": [
    "# calculate how many rows have fewer than x number of words in rixe sample\n",
    "\n",
    "csv_file = '../data/annotated_dataset_deduped.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "word_limit = 15\n",
    "\n",
    "if 'text' in df.columns:\n",
    "    word_counts = df['text'].fillna('').apply(lambda x: len(x.split()))\n",
    "    percent_under_x = (word_counts < word_limit).mean() * 100\n",
    "    print(f\"Percentage of rows with < {word_limit} words: {percent_under_x:.2f}%\")\n",
    "else:\n",
    "    print(\"'text' not found in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bbe5e65-d9a6-4b80-b60a-ad22fd0e792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:26<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall percentage of rows with < 5 words: 5.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate how many rows have fewer than x number of words in italiens/belges sample\n",
    "\n",
    "folder_path = '/store/retronews/etrangers'\n",
    "csv_files = [f for f in glob.glob(os.path.join(folder_path, '*.csv')) if not f.endswith(\"pattern.csv\")]\n",
    "\n",
    "total_rows = 0\n",
    "percent_under_x = 0\n",
    "word_limit = 5\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"processing files\"):\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8', low_memory=False)\n",
    "        if 'text' in df.columns and not df.empty:\n",
    "            df['text'] = df['text'].fillna('')\n",
    "            word_counts = df['text'].apply(lambda x: len(x.split()))\n",
    "            percent_under_x += (word_counts < word_limit).sum()\n",
    "            total_rows += len(word_counts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "if total_rows > 0:\n",
    "    percent_under_10 = (percent_under_x / total_rows) * 100\n",
    "    print(f\"\\nOverall percentage of rows with < {word_limit} words: {percent_under_10:.2f}%\")\n",
    "else:\n",
    "    print(\"no valid rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3690b116-a784-4a18-a650-86e230b7e760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of overlapping events 0.5681818181818182\n"
     ]
    }
   ],
   "source": [
    "# calculate the percentage of events which are relevant in the rixe sample but also present in the archives database\n",
    "\n",
    "rixes_path = '../data/annotated_dataset_deduped.csv'\n",
    "archives_path = '../data/exploded_archives_annotated.csv'\n",
    "\n",
    "df1 = pd.read_csv(rixes_path)\n",
    "df2 = pd.read_csv(archives_path)\n",
    "\n",
    "if \"uniqueid\" in df1.columns and \"relevant\" in df1.columns and \"retronews_uniqueid\" in df2.columns:\n",
    "    relevant_df1 = df1[df1['relevant'] == 1]\n",
    "    matching_ids = set(df2['retronews_uniqueid'])\n",
    "    matched_relevant = relevant_df1[relevant_df1['uniqueid'].isin(matching_ids)]\n",
    "\n",
    "# calculate percentage of overlap\n",
    "    total_relevant = len(relevant_df1)\n",
    "    matched_count = len(matched_relevant)\n",
    "    percentage_events_overlap = (matched_count / total_relevant) * 100 if total_relevant > 0 else 0\n",
    "\n",
    "    print(f\"Percentage of overlapping events {percentage_events_overlap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59899f3d-6f9e-4d73-b8e7-dd1195eaddfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python futureobs",
   "language": "python",
   "name": "futureobs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
